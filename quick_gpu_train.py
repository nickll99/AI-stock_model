"""å¿«é€ŸGPUè®­ç»ƒè„šæœ¬ - ä½¿ç”¨ç¼“å­˜æ•°æ®"""
import torch
import pandas as pd
import numpy as np
from pathlib import Path
from src.data.cached_loader import FeatureCache
from src.models.lstm_model import LSTMModel
from src.training.trainer import ModelTrainer
from src.training.evaluator import ModelEvaluator
from src.data.preprocessor import DataPreprocessor
import time

def main():
    print("=" * 70)
    print("ğŸš€ å¿«é€ŸGPUè®­ç»ƒ - ä½¿ç”¨ç¼“å­˜æ•°æ®")
    print("=" * 70)
    
    # æ£€æŸ¥GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nè®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    # é…ç½®
    symbol = "000001"  # å¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–è‚¡ç¥¨ä»£ç 
    seq_length = 60
    batch_size = 64
    epochs = 50
    
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  è‚¡ç¥¨ä»£ç : {symbol}")
    print(f"  åºåˆ—é•¿åº¦: {seq_length}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  è®­ç»ƒè½®æ•°: {epochs}")
    
    # 1. ä»ç¼“å­˜åŠ è½½ç‰¹å¾æ•°æ®
    print(f"\n{'='*70}")
    print("ğŸ“‚ ä»ç¼“å­˜åŠ è½½ç‰¹å¾æ•°æ®...")
    print(f"{'='*70}")
    
    cache = FeatureCache(cache_dir="data/features")
    df_features = cache.load(symbol)
    
    if df_features is None or len(df_features) == 0:
        print(f"âŒ æœªæ‰¾åˆ° {symbol} çš„ç¼“å­˜æ•°æ®")
        print(f"\nå¯ç”¨çš„ç¼“å­˜è‚¡ç¥¨:")
        cache_files = list(Path("data/features").glob("*_features.parquet"))
        for i, f in enumerate(cache_files[:10], 1):
            stock_code = f.stem.replace("_features", "")
            print(f"  {i}. {stock_code}")
        if len(cache_files) > 10:
            print(f"  ... è¿˜æœ‰ {len(cache_files) - 10} åªè‚¡ç¥¨")
        return
    
    print(f"âœ“ åŠ è½½æˆåŠŸ: {len(df_features)} æ¡è®°å½•")
    print(f"âœ“ ç‰¹å¾æ•°é‡: {len(df_features.columns)} ä¸ª")
    print(f"âœ“ æ•°æ®èŒƒå›´: {df_features.index[0]} è‡³ {df_features.index[-1]}")
    
    # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
    print(f"\n{'='*70}")
    print("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    print(f"{'='*70}")
    
    # æ’é™¤éç‰¹å¾åˆ—
    exclude_cols = ['symbol']
    if 'symbol' in df_features.columns:
        df_features = df_features.drop(columns=['symbol'])
    
    # ç¡®ä¿æœ‰closeåˆ—
    if 'close' not in df_features.columns:
        print("âŒ æ•°æ®ä¸­æ²¡æœ‰closeåˆ—")
        return
    
    # åˆ’åˆ†æ•°æ®é›†
    n_samples = len(df_features)
    train_end = int(n_samples * 0.7)
    val_end = int(n_samples * 0.85)
    
    train_df = df_features.iloc[:train_end].copy()
    val_df = df_features.iloc[train_end:val_end].copy()
    test_df = df_features.iloc[val_end:].copy()
    
    print(f"âœ“ è®­ç»ƒé›†: {len(train_df)} æ¡")
    print(f"âœ“ éªŒè¯é›†: {len(val_df)} æ¡")
    print(f"âœ“ æµ‹è¯•é›†: {len(test_df)} æ¡")
    
    # 3. åˆ›å»ºåºåˆ—æ•°æ®
    preprocessor = DataPreprocessor()
    
    # æ ‡å‡†åŒ–è®­ç»ƒé›†
    feature_cols = [col for col in train_df.columns if col != 'close']
    train_df_norm = preprocessor.normalize_features(train_df, method='standard', fit=True)
    
    # åˆ›å»ºè®­ç»ƒåºåˆ—
    X_train, y_train = preprocessor.create_sequences(
        train_df_norm,
        seq_length=seq_length,
        target_col='close',
        feature_cols=feature_cols
    )
    
    # æ ‡å‡†åŒ–éªŒè¯é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„scalerï¼‰
    val_df_norm = preprocessor.normalize_features(val_df, method='standard', fit=False)
    X_val, y_val = preprocessor.create_sequences(
        val_df_norm,
        seq_length=seq_length,
        target_col='close',
        feature_cols=feature_cols
    )
    
    # æ ‡å‡†åŒ–æµ‹è¯•é›†
    test_df_norm = preprocessor.normalize_features(test_df, method='standard', fit=False)
    X_test, y_test = preprocessor.create_sequences(
        test_df_norm,
        seq_length=seq_length,
        target_col='close',
        feature_cols=feature_cols
    )
    
    print(f"\nâœ“ åºåˆ—æ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"  è®­ç»ƒ: X={X_train.shape}, y={y_train.shape}")
    print(f"  éªŒè¯: X={X_val.shape}, y={y_val.shape}")
    print(f"  æµ‹è¯•: X={X_test.shape}, y={y_test.shape}")
    
    # 4. åˆ›å»ºæ¨¡å‹
    print(f"\n{'='*70}")
    print("ğŸ¤– åˆ›å»ºLSTMæ¨¡å‹...")
    print(f"{'='*70}")
    
    input_size = X_train.shape[2]
    model = LSTMModel(
        input_size=input_size,
        hidden_size=128,
        num_layers=2,
        dropout=0.2
    )
    
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  è¾“å…¥ç»´åº¦: {input_size}")
    print(f"  éšè—å±‚å¤§å°: 128")
    print(f"  å±‚æ•°: 2")
    
    # 5. è®­ç»ƒæ¨¡å‹
    print(f"\n{'='*70}")
    print("ğŸ‹ï¸  å¼€å§‹è®­ç»ƒ...")
    print(f"{'='*70}")
    
    trainer = ModelTrainer(
        model=model,
        device=device,
        learning_rate=0.001,
        batch_size=batch_size
    )
    
    start_time = time.time()
    
    history = trainer.train(
        X_train=X_train,
        y_train=y_train,
        X_val=X_val,
        y_val=y_val,
        epochs=epochs,
        verbose=True
    )
    
    training_time = time.time() - start_time
    
    print(f"\nâœ“ è®­ç»ƒå®Œæˆ!")
    print(f"  æ€»è€—æ—¶: {training_time:.2f} ç§’")
    print(f"  å¹³å‡æ¯è½®: {training_time/epochs:.2f} ç§’")
    
    # 6. è¯„ä¼°æ¨¡å‹
    print(f"\n{'='*70}")
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
    print(f"{'='*70}")
    
    evaluator = ModelEvaluator(model=model, device=device)
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_metrics = evaluator.evaluate(X_test, y_test)
    
    print(f"\næµ‹è¯•é›†ç»“æœ:")
    print(f"  MSE:  {test_metrics['mse']:.6f}")
    print(f"  RMSE: {test_metrics['rmse']:.6f}")
    print(f"  MAE:  {test_metrics['mae']:.6f}")
    print(f"  RÂ²:   {test_metrics['r2']:.6f}")
    
    # 7. ä¿å­˜æ¨¡å‹
    output_dir = Path(f"out/{symbol}_lstm_quick")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = output_dir / "model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'input_size': input_size,
        'hidden_size': 128,
        'num_layers': 2,
        'seq_length': seq_length,
        'feature_cols': feature_cols,
        'scaler': preprocessor.scaler,
        'metrics': test_metrics,
        'history': history
    }, model_path)
    
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    print(f"\n{'='*70}")
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"{'='*70}")
    
    # æ˜¾ç¤ºè®­ç»ƒå†å²
    print(f"\nè®­ç»ƒå†å²:")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {min(history['val_loss']):.6f} (ç¬¬ {history['val_loss'].index(min(history['val_loss']))+1} è½®)")
    print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_loss'][-1]:.6f}")
    print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {history['val_loss'][-1]:.6f}")

if __name__ == "__main__":
    main()
