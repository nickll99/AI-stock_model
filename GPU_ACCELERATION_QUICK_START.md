# GPUåŠ é€Ÿå¿«é€Ÿå¼€å§‹

## âš¡ ä¸€é”®åŠ é€Ÿå‘½ä»¤

```bash
# æè‡´æ€§èƒ½é…ç½®ï¼ˆæ¨èï¼‰
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 512 \
    --hidden-size 256 \
    --device cuda \
    --amp \
    --num-workers 4 \
    --pin-memory
```

**æ•ˆæœï¼š** è®­ç»ƒé€Ÿåº¦æå‡3-4å€ï¼

---

## ğŸ¯ å…³é”®å‚æ•°

| å‚æ•° | ä½œç”¨ | æ•ˆæœ |
|------|------|------|
| `--amp` | æ··åˆç²¾åº¦è®­ç»ƒ | é€Ÿåº¦+2-3xï¼Œæ˜¾å­˜-50% |
| `--batch-size 512` | å¢å¤§æ‰¹æ¬¡ | å……åˆ†åˆ©ç”¨GPU |
| `--num-workers 4` | å¤šè¿›ç¨‹åŠ è½½ | é¿å…GPUç­‰å¾… |
| `--pin-memory` | å›ºå®šå†…å­˜ | åŠ é€Ÿæ•°æ®ä¼ è¾“ |

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é…ç½® | è®­ç»ƒæ—¶é—´ | æå‡ |
|------|---------|------|
| åŸºç¡€é…ç½® | 4å°æ—¶ | 1x |
| +AMP | 2å°æ—¶ | 2x |
| +å¤§Batch | 1.5å°æ—¶ | 2.7x |
| +Workers | 1.2å°æ—¶ | 3.3x |
| **æè‡´é…ç½®** | **1å°æ—¶** | **4x** |

---

## ğŸ” å¦‚ä½•ç¡®è®¤åŠ é€Ÿç”Ÿæ•ˆï¼Ÿ

### 1. æŸ¥çœ‹é…ç½®è¾“å‡º

```
é…ç½®:
  ...
  æ··åˆç²¾åº¦: å¯ç”¨          â† ç¡®è®¤AMPå¯ç”¨
  DataLoader workers: 4   â† ç¡®è®¤å¤šè¿›ç¨‹
```

### 2. ç›‘æ§GPUä½¿ç”¨

```bash
watch -n 1 nvidia-smi
```

**ç›®æ ‡çŠ¶æ€ï¼š**
- GPUåˆ©ç”¨ç‡ï¼š90-100% âœ…
- æ˜¾å­˜ä½¿ç”¨ï¼š70-90% âœ…

### 3. è§‚å¯Ÿè®­ç»ƒé€Ÿåº¦

```
Epoch [1/100] - ... Time: 25.34s  â† åº”è¯¥åœ¨20-30ç§’
```

---

## ğŸ’¡ æ ¹æ®æ˜¾å­˜é€‰æ‹©é…ç½®

### 8GBæ˜¾å­˜

```bash
python scripts/train_universal_model.py \
    --batch-size 256 \
    --hidden-size 128 \
    --amp \
    --num-workers 2 \
    --device cuda
```

### 12GBæ˜¾å­˜

```bash
python scripts/train_universal_model.py \
    --batch-size 512 \
    --hidden-size 256 \
    --amp \
    --num-workers 4 \
    --device cuda
```

### 16GB+æ˜¾å­˜

```bash
python scripts/train_universal_model.py \
    --batch-size 1024 \
    --hidden-size 256 \
    --amp \
    --num-workers 8 \
    --device cuda
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

```bash
# å‡å°batch size
python scripts/train_universal_model.py \
    --batch-size 128 \
    --amp \
    --device cuda
```

### Q2: GPUåˆ©ç”¨ç‡ä½æ€ä¹ˆåŠï¼Ÿ

```bash
# å¢å¤§batch size + å¤šworker
python scripts/train_universal_model.py \
    --batch-size 512 \
    --amp \
    --num-workers 4 \
    --device cuda
```

### Q3: é€Ÿåº¦æ²¡æœ‰æå‡ï¼Ÿ

**æ£€æŸ¥æ¸…å•ï¼š**
1. âœ… æ˜¯å¦å¯ç”¨äº† `--amp`ï¼Ÿ
2. âœ… æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜æ•°æ®ï¼Ÿ
3. âœ… Batch sizeæ˜¯å¦è¶³å¤Ÿå¤§ï¼Ÿ
4. âœ… GPUåˆ©ç”¨ç‡æ˜¯å¦æ¥è¿‘100%ï¼Ÿ

---

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- **docs/GPUè®­ç»ƒåŠ é€ŸæŒ‡å—.md** - å®Œæ•´åŠ é€ŸæŒ‡å—
- **docs/GPUé…ç½®å®Œæ•´æŒ‡å—.md** - GPUé…ç½®è¯´æ˜

---

## âœ¨ å¿«é€Ÿå¼€å§‹

```bash
# 1. ç¡®ä¿æ•°æ®å·²é¢„çƒ­
python scripts/prepare_training_data.py --symbols all --workers 8 --resume

# 2. ä½¿ç”¨åŠ é€Ÿé…ç½®è®­ç»ƒ
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 512 \
    --hidden-size 256 \
    --device cuda \
    --amp \
    --num-workers 4 \
    --pin-memory

# 3. ç›‘æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi
```

ç°åœ¨ä½ çš„è®­ç»ƒé€Ÿåº¦ä¼šå¿«3-4å€ï¼ğŸš€
