# è®­ç»ƒé…ç½®è¯¦è§£

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### ç¬¬1æ­¥ï¼šè¯Šæ–­ç¯å¢ƒ

```bash
python diagnose_gpu.py
```

### ç¬¬2æ­¥ï¼šé€‰æ‹©é…ç½®

æ ¹æ®è¯Šæ–­ç»“æœé€‰æ‹©åˆé€‚çš„é…ç½®ã€‚

---

## ğŸ“‹ æ‰€æœ‰å¯ç”¨å‚æ•°

### æ¨¡å‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|--------|------|--------|
| `--model-type` | lstm | æ¨¡å‹ç±»å‹ (lstm/gru/transformer) | transformer |
| `--hidden-size` | 128 | éšè—å±‚å¤§å° | 128-256 |
| `--num-layers` | 2 | ç½‘ç»œå±‚æ•° | 2-4 |
| `--dropout` | 0.2 | Dropoutç‡ | 0.1-0.3 |
| `--stock-embedding-dim` | 32 | è‚¡ç¥¨åµŒå…¥ç»´åº¦ | 32-64 |

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|--------|------|--------|
| `--epochs` | 50 | è®­ç»ƒè½®æ•° | 50-100 |
| `--batch-size` | 128 | æ‰¹æ¬¡å¤§å° | 128-512 |
| `--learning-rate` | 0.001 | å­¦ä¹ ç‡ | 0.0001-0.001 |

### æ€§èƒ½å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|--------|------|--------|
| `--device` | cuda/cpu | è®¡ç®—è®¾å¤‡ | cuda |
| `--amp` | False | æ··åˆç²¾åº¦è®­ç»ƒ | å¯ç”¨ï¼ˆGPUï¼‰ |
| `--num-workers` | 4 | DataLoaderè¿›ç¨‹æ•° | 0ï¼ˆAMD GPUï¼‰ |
| `--pin-memory` | True | å›ºå®šå†…å­˜ | Trueï¼ˆGPUï¼‰ |
| `--gradient-accumulation-steps` | 1 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 1-4 |

### æ•°æ®å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|--------|------|--------|
| `--limit` | None | é™åˆ¶è‚¡ç¥¨æ•°é‡ | 500-1000ï¼ˆæµ‹è¯•ï¼‰ |
| `--stock-type` | None | è‚¡ç¥¨ç±»å‹ç­›é€‰ | None |
| `--no-cache` | False | ä¸ä½¿ç”¨ç¼“å­˜ | False |
| `--kline-cache-dir` | data/parquet | Kçº¿ç¼“å­˜ç›®å½• | data/parquet |
| `--feature-cache-dir` | data/features | ç‰¹å¾ç¼“å­˜ç›®å½• | data/features |

### è¾“å‡ºå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|--------|------|--------|
| `--output-dir` | out/universal_model | è¾“å‡ºç›®å½• | è‡ªå®šä¹‰ |

---

## ğŸ¨ é¢„è®¾é…ç½®

### é…ç½®1ï¼šå¿«é€Ÿæµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** éªŒè¯ç¯å¢ƒå’Œä»£ç 

```bash
python scripts/train_universal_model.py \
    --model-type lstm \
    --epochs 5 \
    --batch-size 128 \
    --hidden-size 64 \
    --device cuda \
    --num-workers 0 \
    --limit 100
```

**é¢„æœŸç»“æœï¼š**
- è®­ç»ƒæ—¶é—´ï¼š5åˆ†é’Ÿ
- å†…å­˜å ç”¨ï¼š2-4GB
- GPUæ˜¾å­˜ï¼š1-2GB

### é…ç½®2ï¼šä¸­ç­‰è§„æ¨¡ï¼ˆ30åˆ†é’Ÿï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** æ¨¡å‹è°ƒè¯•å’Œå‚æ•°è°ƒä¼˜

```bash
python scripts/train_universal_model.py \
    --model-type lstm \
    --epochs 20 \
    --batch-size 256 \
    --hidden-size 128 \
    --device cuda \
    --num-workers 0 \
    --limit 500
```

**é¢„æœŸç»“æœï¼š**
- è®­ç»ƒæ—¶é—´ï¼š30åˆ†é’Ÿ
- å†…å­˜å ç”¨ï¼š8-12GB
- GPUæ˜¾å­˜ï¼š4-6GB

### é…ç½®3ï¼šæ ‡å‡†è®­ç»ƒï¼ˆ2å°æ—¶ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** æ­£å¼è®­ç»ƒ

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 50 \
    --batch-size 256 \
    --hidden-size 128 \
    --device cuda \
    --num-workers 0 \
    --limit 1000
```

**é¢„æœŸç»“æœï¼š**
- è®­ç»ƒæ—¶é—´ï¼š2å°æ—¶
- å†…å­˜å ç”¨ï¼š16-24GB
- GPUæ˜¾å­˜ï¼š8-12GB

### é…ç½®4ï¼šå…¨é‡è®­ç»ƒï¼ˆ4-6å°æ—¶ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** ç”Ÿäº§ç¯å¢ƒ

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 256 \
    --hidden-size 256 \
    --device cuda \
    --num-workers 0
```

**é¢„æœŸç»“æœï¼š**
- è®­ç»ƒæ—¶é—´ï¼š4-6å°æ—¶
- å†…å­˜å ç”¨ï¼š32-48GB
- GPUæ˜¾å­˜ï¼š12-16GB

### é…ç½®5ï¼šå†…å­˜å—é™ï¼ˆæ¨èAMD GPUï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** å†…å­˜æˆ–æ˜¾å­˜ä¸è¶³

```bash
python scripts/train_universal_model.py \
    --model-type lstm \
    --epochs 50 \
    --batch-size 128 \
    --hidden-size 128 \
    --device cuda \
    --num-workers 0 \
    --limit 1000
```

**é¢„æœŸç»“æœï¼š**
- è®­ç»ƒæ—¶é—´ï¼š2-3å°æ—¶
- å†…å­˜å ç”¨ï¼š12-16GB
- GPUæ˜¾å­˜ï¼š4-6GB

### é…ç½®6ï¼šé«˜æ€§èƒ½ï¼ˆNVIDIA GPUï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** NVIDIA GPU + å……è¶³èµ„æº

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 512 \
    --hidden-size 256 \
    --device cuda \
    --amp \
    --num-workers 4 \
    --pin-memory
```

**é¢„æœŸç»“æœï¼š**
- è®­ç»ƒæ—¶é—´ï¼š1-2å°æ—¶
- å†…å­˜å ç”¨ï¼š16-24GB
- GPUæ˜¾å­˜ï¼š12-16GB

---

## ğŸ’¡ å‚æ•°è°ƒä¼˜æŒ‡å—

### Batch Sizeé€‰æ‹©

| æ˜¾å­˜å¤§å° | æ¨èBatch Size | è¯´æ˜ |
|---------|---------------|------|
| 4GB | 64-128 | å°æ¨¡å‹ |
| 8GB | 128-256 | æ ‡å‡†é…ç½® |
| 12GB | 256-512 | é«˜æ€§èƒ½ |
| 16GB+ | 512-1024 | æè‡´æ€§èƒ½ |

**è§„åˆ™ï¼š**
- æ˜¾å­˜ä¸è¶³ï¼šå‡å°batch size
- GPUåˆ©ç”¨ç‡ä½ï¼šå¢å¤§batch size
- å†…å­˜ä¸è¶³ï¼šå‡å°batch sizeæˆ–é™åˆ¶è‚¡ç¥¨æ•°é‡

### Hidden Sizeé€‰æ‹©

| æ¨¡å‹å¤æ‚åº¦ | Hidden Size | å‚æ•°é‡ | è¯´æ˜ |
|-----------|-------------|--------|------|
| å° | 64 | ~500K | å¿«é€Ÿè®­ç»ƒ |
| ä¸­ | 128 | ~2M | æ ‡å‡†é…ç½® |
| å¤§ | 256 | ~8M | é«˜ç²¾åº¦ |
| è¶…å¤§ | 512 | ~32M | ç ”ç©¶ç”¨ |

**è§„åˆ™ï¼š**
- æ•°æ®é‡å¤§ï¼šä½¿ç”¨æ›´å¤§çš„hidden size
- æ˜¾å­˜ä¸è¶³ï¼šä½¿ç”¨æ›´å°çš„hidden size
- è¿‡æ‹Ÿåˆï¼šå‡å°hidden sizeæˆ–å¢åŠ dropout

### Epochsé€‰æ‹©

| åœºæ™¯ | Epochs | è¯´æ˜ |
|------|--------|------|
| å¿«é€Ÿæµ‹è¯• | 5-10 | éªŒè¯ä»£ç  |
| è°ƒè¯• | 20-30 | å‚æ•°è°ƒä¼˜ |
| æ ‡å‡†è®­ç»ƒ | 50-100 | æ­£å¼è®­ç»ƒ |
| ç²¾ç»†è®­ç»ƒ | 100-200 | è¿½æ±‚æè‡´ |

**è§„åˆ™ï¼š**
- ä½¿ç”¨æ—©åœï¼ˆearly stoppingï¼‰é¿å…è¿‡æ‹Ÿåˆ
- è§‚å¯ŸéªŒè¯æŸå¤±ï¼Œåœæ­¢æ—¶æœºåˆé€‚å³å¯

### Learning Rateé€‰æ‹©

| æ¨¡å‹ç±»å‹ | Learning Rate | è¯´æ˜ |
|---------|--------------|------|
| LSTM/GRU | 0.001 | æ ‡å‡† |
| Transformer | 0.0001-0.0005 | è¾ƒå° |
| å¤§æ¨¡å‹ | 0.0001 | æ›´å° |

**è§„åˆ™ï¼š**
- æŸå¤±ä¸ä¸‹é™ï¼šå‡å°learning rate
- è®­ç»ƒä¸ç¨³å®šï¼šå‡å°learning rate
- æ”¶æ•›å¤ªæ…¢ï¼šå¢å¤§learning rate

---

## ğŸ”§ å†…å­˜ä¼˜åŒ–ç­–ç•¥

### é—®é¢˜ï¼šå†…å­˜å ç”¨50%

**åŸå› ï¼š** æ•°æ®åŠ è½½é˜¶æ®µä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è‚¡ç¥¨

**è§£å†³æ–¹æ¡ˆï¼š**

#### 1. é™åˆ¶è‚¡ç¥¨æ•°é‡

```bash
# ä»500åªå¼€å§‹
--limit 500

# é€æ­¥å¢åŠ 
--limit 1000
--limit 2000
```

#### 2. å‡å°Batch Size

```bash
# ä»128å¼€å§‹
--batch-size 128

# å¦‚æœè¿˜ä¸å¤Ÿ
--batch-size 64
```

#### 3. ç¦ç”¨DataLoader Workers

```bash
# é¿å…å¤šè¿›ç¨‹å†…å­˜å¤åˆ¶
--num-workers 0
```

#### 4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

```bash
--model-type lstm \
--hidden-size 64
```

#### 5. åˆ†æ‰¹è®­ç»ƒ

```bash
# ç¬¬ä¸€æ‰¹
python scripts/train_universal_model.py \
    --limit 1000 \
    --output-dir out/batch1

# ç¬¬äºŒæ‰¹
python scripts/train_universal_model.py \
    --limit 2000 \
    --output-dir out/batch2
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### GPUç›‘æ§

```bash
# NVIDIA GPU
watch -n 1 nvidia-smi

# AMD GPU
watch -n 1 rocm-smi
```

### å†…å­˜ç›‘æ§

```bash
# ç³»ç»Ÿå†…å­˜
watch -n 1 free -h

# è¿›ç¨‹å†…å­˜
watch -n 1 "ps aux | grep train_universal_model | grep -v grep"
```

### è®­ç»ƒæ—¥å¿—

å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š
- æ¯ä¸ªepochæ—¶é—´ï¼ˆåº”è¯¥ç¨³å®šï¼‰
- è®­ç»ƒæŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
- éªŒè¯æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
- GPUåˆ©ç”¨ç‡ï¼ˆåº”è¯¥>80%ï¼‰

---

## ğŸ¯ æ¨èå·¥ä½œæµ

### ç¬¬1æ­¥ï¼šç¯å¢ƒè¯Šæ–­

```bash
python diagnose_gpu.py
```

### ç¬¬2æ­¥ï¼šå¿«é€Ÿæµ‹è¯•

```bash
python scripts/train_universal_model.py \
    --device cuda \
    --limit 100 \
    --epochs 5 \
    --num-workers 0
```

### ç¬¬3æ­¥ï¼šä¸­ç­‰è§„æ¨¡

```bash
python scripts/train_universal_model.py \
    --device cuda \
    --limit 500 \
    --epochs 20 \
    --batch-size 256 \
    --num-workers 0
```

### ç¬¬4æ­¥ï¼šå…¨é‡è®­ç»ƒ

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 256 \
    --hidden-size 256 \
    --device cuda \
    --num-workers 0
```

---

## âœ¨ æ€»ç»“

### AMD GPUæ¨èé…ç½®

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 256 \
    --hidden-size 256 \
    --device cuda \
    --num-workers 0 \
    --limit 1000
```

### NVIDIA GPUæ¨èé…ç½®

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 512 \
    --hidden-size 256 \
    --device cuda \
    --amp \
    --num-workers 4 \
    --pin-memory
```

### å†…å­˜å—é™é…ç½®

```bash
python scripts/train_universal_model.py \
    --model-type lstm \
    --epochs 50 \
    --batch-size 128 \
    --hidden-size 128 \
    --device cuda \
    --num-workers 0 \
    --limit 500
```

ç°åœ¨ä½ æœ‰å®Œæ•´çš„è®­ç»ƒé…ç½®æŒ‡å—äº†ï¼ğŸš€
