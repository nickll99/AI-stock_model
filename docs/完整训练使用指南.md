# 完整训练使用指南

## 📋 目录

1. [环境准备](#环境准备)
2. [数据预热](#数据预热)
3. [模型训练](#模型训练)
4. [GPU加速配置](#gpu加速配置)
5. [训练参数调优](#训练参数调优)
6. [常见问题](#常见问题)

---

## 环境准备

### 1. 检查Python环境

```bash
python --version  # 需要 Python 3.8+
```

### 2. 安装依赖

```bash
pip install -r requirements.txt
```

### 3. 检查PyTorch和CUDA

```bash
python -c "import torch; print(f'PyTorch版本: {torch.__version__}'); print(f'CUDA可用: {torch.cuda.is_available()}'); print(f'CUDA版本: {torch.version.cuda if torch.cuda.is_available() else \"N/A\"}')"
```

**输出示例：**
```
PyTorch版本: 2.0.1
CUDA可用: True
CUDA版本: 11.8
```

### 4. 配置数据库

确保`.env`文件配置正确：

```bash
# 复制示例配置
cp .env.example .env

# 编辑配置
vim .env
```

**必需配置：**
```env
# 数据库配置
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_password
DB_NAME=stock_ai

# Tushare配置（用于数据获取）
TUSHARE_TOKEN=your_tushare_token
```

### 5. 验证配置

```bash
python check_config.py
```

---

## 数据预热

### 为什么需要数据预热？

数据预热可以：
- ✅ 提前计算和缓存特征数据
- ✅ 大幅提升训练速度（10-50倍）
- ✅ 降低数据库负载
- ✅ 支持断点续传

### 基本用法

#### 1. 预热单只股票（测试）

```bash
python scripts/prepare_training_data.py --symbols 000001 --workers 1
```

#### 2. 预热多只股票

```bash
python scripts/prepare_training_data.py --symbols 000001,600519,000858 --workers 2
```

#### 3. 预热所有股票（推荐）

```bash
# 使用4个进程并发 + 断点续传
python scripts/prepare_training_data.py --symbols all --workers 4 --resume
```

### 高级用法

#### 分阶段预热（推荐大规模数据）

```bash
# 第一阶段：快速预热K线数据（IO密集型，可以用更多进程）
python scripts/prepare_training_data.py \
    --symbols all \
    --kline-only \
    --workers 8 \
    --resume

# 第二阶段：计算特征数据（CPU密集型，进程数=CPU核心数）
python scripts/prepare_training_data.py \
    --symbols all \
    --features-only \
    --workers 4 \
    --resume
```

#### 自定义日期范围

```bash
python scripts/prepare_training_data.py \
    --symbols all \
    --start-date 2020-01-01 \
    --end-date 2024-12-31 \
    --workers 4 \
    --resume
```

#### 后台运行（Linux/Mac）

```bash
nohup python scripts/prepare_training_data.py \
    --symbols all \
    --workers 4 \
    --resume \
    > logs/preheating_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# 查看进度
tail -f logs/preheating_*.log
```

### 性能参考

| 股票数量 | 单进程 | 4进程 | 8进程 | 推荐配置 |
|---------|-------|-------|-------|----------|
| 10只 | 2分钟 | 1分钟 | 30秒 | 2进程 |
| 100只 | 20分钟 | 6分钟 | 3分钟 | 4进程 |
| 1000只 | 200分钟 | 60分钟 | 30分钟 | 6进程 |
| 5000只 | 1000分钟 | 300分钟 | 150分钟 | 8进程 |

---

## 模型训练

### 快速开始

#### 1. 使用默认配置训练

```bash
python examples/train_with_manager.py
```

**默认配置：**
- 股票代码：000001
- 模型类型：LSTM
- 序列长度：60
- 训练轮数：50
- 批次大小：32
- 学习率：0.001

#### 2. 训练输出

训练过程会显示：
```
======================================================================
  标准化训练流程示例
  数据来源: MySQL
  输出目录: out/
======================================================================

训练配置:
  股票代码: 000001
  模型类型: lstm
  序列长度: 60
  训练轮数: 50

初始化训练管理器...
✓ 训练运行: 000001_lstm_20241119_031440
✓ 输出目录: out/000001_lstm_20241119_031440

从MySQL加载数据...
✓ 数据范围: 2022-01-04 至 2025-11-18
✓ 加载数据: 726 条记录

构建特征...
原始数据: 726 条记录
添加技术指标后: 726 条记录
添加价格特征后: 726 条记录
添加成交量特征后: 726 条记录

删除完全是NaN的列 (3个):
  - pe
  - pb
  - turnover_rate

包含NaN的列 (15个):
  ma_60: 59 个NaN (8.1%)
  ema_26: 25 个NaN (3.4%)
  ...

使用填充策略后: 726 条记录
✓ 训练集: (393, 60, 48)
✓ 验证集: (84, 60, 48)
✓ 测试集: (84, 60, 48)
✓ 特征数量: 48

创建模型...
✓ 模型: LSTMModel
✓ 参数量: 125,953

开始训练，设备: cpu
训练样本数: 393, 验证样本数: 84

Epoch [1/50] - Train Loss: 0.440388, Val Loss: 106.615879, Time: 0.48s
检查点已保存: out/000001_lstm_20241119_031440/checkpoints/best_model.pth
Epoch [2/50] - Train Loss: 0.110346, Val Loss: 106.388031, Time: 0.29s
...
Epoch [16/50] - Train Loss: 0.020163, Val Loss: 106.104767, Time: 0.26s
早停触发，在第 16 轮停止训练

训练完成！
✓ 训练完成
✓ 最佳验证损失: 104.861324

评估模型...
✓ 评估指标:
  MAE: 0.123456
  RMSE: 0.234567
  MAPE: 1.23%
  R²: 0.95
  方向准确率: 65.5%

保存结果...
✓ 训练历史已保存
✓ 评估指标已保存
✓ 预测结果已保存
✓ 可视化图表已保存

训练完成！输出目录: out/000001_lstm_20241119_031440
```

### 自定义训练配置

#### 修改训练参数

编辑 `examples/train_with_manager.py`：

```python
config = {
    "stock_symbol": "600519",      # 修改股票代码
    "model_type": "gru",           # 修改模型类型: lstm/gru/transformer
    "seq_length": 90,              # 修改序列长度
    "hidden_size": 256,            # 修改隐藏层大小
    "num_layers": 3,               # 修改层数
    "dropout": 0.3,                # 修改dropout
    "epochs": 100,                 # 修改训练轮数
    "batch_size": 64,              # 修改批次大小
    "learning_rate": 0.0005,       # 修改学习率
    "patience": 15,                # 修改早停耐心值
    "data_source": "MySQL",        # 数据源
    "train_start_date": "2020-01-01",  # 修改开始日期
    "train_end_date": "2024-12-31"     # 修改结束日期
}
```

#### 训练不同模型

```bash
# LSTM模型（默认）
python examples/train_with_manager.py

# GRU模型（修改config中的model_type为"gru"）
# Transformer模型（修改config中的model_type为"transformer"）
```

### 训练输出文件

训练完成后，会在`out/`目录下生成以下文件：

```
out/000001_lstm_20241119_031440/
├── checkpoints/                    # 模型检查点
│   ├── best_model.pth             # 最佳模型
│   ├── checkpoint_epoch_10.pth    # 第10轮检查点
│   └── final_model.pth            # 最终模型
├── logs/                          # 训练日志
│   └── training.log               # 详细日志
├── results/                       # 训练结果
│   ├── training_history.json     # 训练历史
│   ├── evaluation_metrics.json   # 评估指标
│   └── predictions.csv           # 预测结果
├── plots/                         # 可视化图表
│   ├── training_history.png      # 训练曲线
│   ├── predictions.png           # 预测对比
│   └── residuals.png             # 残差分析
└── config.json                    # 训练配置
```

---

## GPU加速配置

### 为什么使用GPU？

- ✅ 训练速度提升10-100倍
- ✅ 支持更大的模型和批次
- ✅ 可以训练更复杂的网络

### 检查GPU可用性

```bash
python -c "import torch; print(f'CUDA可用: {torch.cuda.is_available()}'); print(f'GPU数量: {torch.cuda.device_count()}'); print(f'GPU名称: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

**输出示例（有GPU）：**
```
CUDA可用: True
GPU数量: 1
GPU名称: NVIDIA GeForce RTX 3080
```

**输出示例（无GPU）：**
```
CUDA可用: False
GPU数量: 0
GPU名称: N/A
```

### 安装CUDA版本的PyTorch

#### 方法1：使用pip（推荐）

```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### 方法2：使用conda

```bash
# CUDA 11.8
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# CUDA 12.1
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
```

### GPU训练配置

训练代码会自动检测并使用GPU，无需修改代码：

```python
# 代码会自动选择设备
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"开始训练，设备: {device}")
```

### GPU性能对比

| 配置 | 训练时间（50轮） | 相对速度 |
|------|-----------------|----------|
| CPU (8核 i7) | 30分钟 | 1x |
| GPU (GTX 1660) | 5分钟 | 6x |
| GPU (RTX 3080) | 2分钟 | 15x |
| GPU (RTX 4090) | 1分钟 | 30x |

### GPU内存管理

如果遇到GPU内存不足（OOM）错误：

```python
# 方法1：减小批次大小
config = {
    "batch_size": 16,  # 从32减小到16
}

# 方法2：减小模型大小
config = {
    "hidden_size": 64,   # 从128减小到64
    "num_layers": 1,     # 从2减小到1
}

# 方法3：减小序列长度
config = {
    "seq_length": 30,    # 从60减小到30
}
```

### 多GPU训练（高级）

如果有多个GPU，可以使用DataParallel：

```python
# 在train_with_manager.py中添加
if torch.cuda.device_count() > 1:
    print(f"使用 {torch.cuda.device_count()} 个GPU训练")
    model = nn.DataParallel(model)
```

---

## 训练参数调优

### 关键参数说明

#### 1. 序列长度 (seq_length)

**含义**：用于预测的历史数据长度

**推荐值**：
- 短期预测（1-5天）：30-60
- 中期预测（5-20天）：60-90
- 长期预测（20天以上）：90-120

**影响**：
- 太小：无法捕捉长期趋势
- 太大：训练慢，可能过拟合

#### 2. 隐藏层大小 (hidden_size)

**含义**：LSTM/GRU的隐藏单元数量

**推荐值**：
- 小模型：64-128
- 中等模型：128-256
- 大模型：256-512

**影响**：
- 太小：模型容量不足，欠拟合
- 太大：训练慢，容易过拟合

#### 3. 层数 (num_layers)

**含义**：LSTM/GRU的堆叠层数

**推荐值**：
- 简单任务：1-2层
- 复杂任务：2-3层
- 不推荐超过4层

**影响**：
- 太少：模型表达能力不足
- 太多：训练困难，梯度消失

#### 4. Dropout

**含义**：防止过拟合的正则化技术

**推荐值**：
- 小数据集：0.3-0.5
- 大数据集：0.1-0.3
- 无过拟合：0.0

**影响**：
- 太小：容易过拟合
- 太大：欠拟合

#### 5. 学习率 (learning_rate)

**含义**：优化器的步长

**推荐值**：
- Adam优化器：0.0001-0.001
- SGD优化器：0.001-0.01

**影响**：
- 太小：训练慢，可能陷入局部最优
- 太大：训练不稳定，无法收敛

#### 6. 批次大小 (batch_size)

**含义**：每次更新使用的样本数

**推荐值**：
- CPU训练：16-32
- GPU训练：32-128
- 大GPU：128-256

**影响**：
- 太小：训练不稳定，速度慢
- 太大：内存不足，泛化能力差

### 调优策略

#### 策略1：从小模型开始

```python
# 第一步：快速验证
config = {
    "seq_length": 30,
    "hidden_size": 64,
    "num_layers": 1,
    "epochs": 20,
    "batch_size": 32,
}

# 第二步：增加模型容量
config = {
    "seq_length": 60,
    "hidden_size": 128,
    "num_layers": 2,
    "epochs": 50,
    "batch_size": 32,
}

# 第三步：精细调优
config = {
    "seq_length": 90,
    "hidden_size": 256,
    "num_layers": 2,
    "dropout": 0.3,
    "epochs": 100,
    "batch_size": 64,
}
```

#### 策略2：学习率调优

```python
# 尝试不同的学习率
learning_rates = [0.0001, 0.0005, 0.001, 0.005]

for lr in learning_rates:
    config["learning_rate"] = lr
    # 训练并记录结果
```

#### 策略3：早停策略

```python
# 使用早停避免过拟合
config = {
    "patience": 10,  # 验证损失10轮不下降就停止
}
```

### 推荐配置组合

#### 配置1：快速测试（CPU友好）

```python
config = {
    "seq_length": 30,
    "hidden_size": 64,
    "num_layers": 1,
    "dropout": 0.2,
    "epochs": 30,
    "batch_size": 16,
    "learning_rate": 0.001,
    "patience": 5,
}
```

**适用场景**：快速验证想法，CPU训练

#### 配置2：标准配置（推荐）

```python
config = {
    "seq_length": 60,
    "hidden_size": 128,
    "num_layers": 2,
    "dropout": 0.2,
    "epochs": 50,
    "batch_size": 32,
    "learning_rate": 0.001,
    "patience": 10,
}
```

**适用场景**：日常训练，CPU或GPU

#### 配置3：高性能配置（GPU推荐）

```python
config = {
    "seq_length": 90,
    "hidden_size": 256,
    "num_layers": 3,
    "dropout": 0.3,
    "epochs": 100,
    "batch_size": 64,
    "learning_rate": 0.0005,
    "patience": 15,
}
```

**适用场景**：追求最佳性能，GPU训练

---

## 常见问题

### Q1: 训练时显示使用CPU，如何使用GPU？

**原因**：
1. 没有安装CUDA版本的PyTorch
2. CUDA驱动版本不匹配
3. GPU不可用

**解决方案**：

```bash
# 1. 检查CUDA是否可用
python -c "import torch; print(torch.cuda.is_available())"

# 2. 如果返回False，重新安装CUDA版本的PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 3. 检查NVIDIA驱动
nvidia-smi
```

### Q2: 训练速度很慢怎么办？

**解决方案**：

1. **使用数据预热**
```bash
python scripts/prepare_training_data.py --symbols all --workers 4 --resume
```

2. **使用GPU训练**
```bash
# 安装CUDA版本的PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

3. **减小模型大小**
```python
config = {
    "seq_length": 30,    # 从60减小到30
    "hidden_size": 64,   # 从128减小到64
    "batch_size": 64,    # 增大批次
}
```

### Q3: 出现内存不足（OOM）错误

**解决方案**：

```python
# 减小批次大小
config = {
    "batch_size": 16,  # 从32减小到16
}

# 或减小模型大小
config = {
    "hidden_size": 64,   # 从128减小到64
    "num_layers": 1,     # 从2减小到1
}
```

### Q4: 模型过拟合怎么办？

**症状**：训练损失很低，但验证损失很高

**解决方案**：

```python
# 1. 增加Dropout
config = {
    "dropout": 0.5,  # 从0.2增加到0.5
}

# 2. 使用早停
config = {
    "patience": 5,  # 减小耐心值
}

# 3. 增加训练数据
config = {
    "train_start_date": "2018-01-01",  # 使用更多历史数据
}

# 4. 减小模型容量
config = {
    "hidden_size": 64,   # 从128减小到64
    "num_layers": 1,     # 从2减小到1
}
```

### Q5: 模型欠拟合怎么办？

**症状**：训练损失和验证损失都很高

**解决方案**：

```python
# 1. 增加模型容量
config = {
    "hidden_size": 256,  # 从128增加到256
    "num_layers": 3,     # 从2增加到3
}

# 2. 减小Dropout
config = {
    "dropout": 0.1,  # 从0.2减小到0.1
}

# 3. 增加训练轮数
config = {
    "epochs": 100,  # 从50增加到100
}

# 4. 调整学习率
config = {
    "learning_rate": 0.001,  # 尝试不同的学习率
}
```

### Q6: 如何查看训练进度？

**方法1：实时查看**
```bash
# 训练会实时显示进度
Epoch [1/50] - Train Loss: 0.440388, Val Loss: 106.615879, Time: 0.48s
```

**方法2：查看日志文件**
```bash
tail -f out/000001_lstm_20241119_031440/logs/training.log
```

**方法3：查看训练曲线**
```bash
# 训练完成后查看
open out/000001_lstm_20241119_031440/plots/training_history.png
```

### Q7: 如何恢复中断的训练？

**解决方案**：

```python
# 在train_with_manager.py中添加
if Path("out/000001_lstm_20241119_031440/checkpoints/best_model.pth").exists():
    model.load_state_dict(torch.load("out/000001_lstm_20241119_031440/checkpoints/best_model.pth"))
    print("从检查点恢复训练")
```

### Q8: 如何批量训练多只股票？

**方法1：使用循环**

```python
# 创建 batch_train.py
stocks = ["000001", "600519", "000858", "002415"]

for stock in stocks:
    config["stock_symbol"] = stock
    # 训练代码...
```

**方法2：使用并行**

```bash
# 使用GNU parallel
parallel python examples/train_with_manager.py --symbol {} ::: 000001 600519 000858
```

---

## 总结

### 完整训练流程

```bash
# 1. 环境准备
python check_config.py

# 2. 数据预热（推荐）
python scripts/prepare_training_data.py --symbols all --workers 4 --resume

# 3. 模型训练
python examples/train_with_manager.py

# 4. 查看结果
ls -lh out/000001_lstm_*/
```

### 性能优化建议

1. **数据预热**：必须使用，速度提升10-50倍
2. **GPU训练**：强烈推荐，速度提升10-100倍
3. **并发处理**：数据预热时使用4-8个进程
4. **批次大小**：GPU训练时使用64-128
5. **早停策略**：避免过拟合，节省时间

### 推荐配置

**CPU训练（快速测试）：**
```python
config = {
    "seq_length": 30,
    "hidden_size": 64,
    "num_layers": 1,
    "batch_size": 16,
    "epochs": 30,
}
```

**GPU训练（生产环境）：**
```python
config = {
    "seq_length": 60,
    "hidden_size": 128,
    "num_layers": 2,
    "batch_size": 64,
    "epochs": 50,
}
```

现在你已经掌握了完整的训练流程！🚀

如有问题，请参考：
- [数据预热指南](./并发数据预热使用指南.md)
- [配置指南](./CONFIGURATION_GUIDE.md)
- [API文档](./API_DOCUMENTATION.md)
