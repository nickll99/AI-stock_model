# AMD GPU (ROCm) è®­ç»ƒé…ç½®æŒ‡å—

## ğŸ¯ é‡è¦è¯´æ˜

**ä½ çš„æœåŠ¡å™¨ä½¿ç”¨AMDæ˜¾å¡ï¼ˆAå¡ï¼‰ï¼Œéœ€è¦ä½¿ç”¨ROCmè€Œä¸æ˜¯CUDAï¼**

---

## ğŸ“‹ å½“å‰é—®é¢˜åˆ†æ

### é—®é¢˜ç°è±¡
- âœ… å†…å­˜å ç”¨50%ï¼ˆæ•°æ®åŠ è½½é˜¶æ®µï¼‰
- âŒ GPUæ²¡æœ‰è¢«ä½¿ç”¨
- âŒ è®­ç»ƒé€Ÿåº¦æ…¢

### æ ¹æœ¬åŸå› 
1. **PyTorchå¯èƒ½æ²¡æœ‰æ­£ç¡®å®‰è£…ROCmç‰ˆæœ¬**
2. **æ•°æ®åŠ è½½é˜¶æ®µå ç”¨å¤§é‡å†…å­˜**
3. **éœ€è¦ä½¿ç”¨ `--device cuda` å‚æ•°ï¼ˆROCmå…¼å®¹CUDA APIï¼‰**

---

## ğŸ”§ è§£å†³æ–¹æ¡ˆ

### ç¬¬1æ­¥ï¼šæ£€æŸ¥PyTorch ROCmæ”¯æŒ

```bash
# æ£€æŸ¥PyTorchæ˜¯å¦æ”¯æŒROCm
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}'); print(f'è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}'); print(f'å½“å‰è®¾å¤‡: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"æ— \"}')"
```

**é¢„æœŸè¾“å‡ºï¼ˆæ­£ç¡®é…ç½®ï¼‰ï¼š**
```
PyTorchç‰ˆæœ¬: 2.0.0+rocm5.4.2
CUDAå¯ç”¨: True
è®¾å¤‡æ•°é‡: 1
å½“å‰è®¾å¤‡: AMD Radeon RX 7900 XTX  # æˆ–å…¶ä»–AMDæ˜¾å¡å‹å·
```

**å¦‚æœè¾“å‡º `CUDAå¯ç”¨: False`ï¼Œéœ€è¦é‡æ–°å®‰è£…PyTorch ROCmç‰ˆæœ¬ï¼**

### ç¬¬2æ­¥ï¼šå®‰è£…PyTorch ROCmç‰ˆæœ¬ï¼ˆå¦‚æœéœ€è¦ï¼‰

```bash
# å¸è½½ç°æœ‰PyTorch
pip uninstall torch torchvision torchaudio -y

# å®‰è£…ROCmç‰ˆæœ¬çš„PyTorch
# æ ¹æ®ä½ çš„ROCmç‰ˆæœ¬é€‰æ‹©ï¼ˆé€šå¸¸æ˜¯5.4æˆ–5.6ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6

# æˆ–è€…ä½¿ç”¨ROCm 5.4
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4
```

### ç¬¬3æ­¥ï¼šéªŒè¯ROCmç¯å¢ƒ

```bash
# æ£€æŸ¥ROCmç‰ˆæœ¬
rocm-smi

# æˆ–è€…
rocminfo | grep "Name:"
```

### ç¬¬4æ­¥ï¼šä¼˜åŒ–è®­ç»ƒé…ç½®

åˆ›å»ºè®­ç»ƒé…ç½®æ–‡ä»¶ `train_config.sh`ï¼š

```bash
#!/bin/bash

# AMD GPUä¼˜åŒ–é…ç½®
export HSA_OVERRIDE_GFX_VERSION=10.3.0  # æ ¹æ®ä½ çš„GPUè°ƒæ•´
export PYTORCH_HIP_ALLOC_CONF=max_split_size_mb:512

# è®­ç»ƒå‘½ä»¤
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 256 \
    --hidden-size 256 \
    --device cuda \
    --amp \
    --num-workers 2 \
    --limit 500
```

---

## ğŸš€ æ¨èè®­ç»ƒé…ç½®

### é…ç½®1ï¼šå†…å­˜ä¼˜åŒ–ï¼ˆæ¨èå…ˆç”¨è¿™ä¸ªï¼‰

```bash
python scripts/train_universal_model.py \
    --model-type lstm \
    --epochs 50 \
    --batch-size 128 \
    --hidden-size 128 \
    --device cuda \
    --num-workers 0 \
    --limit 500
```

**ç‰¹ç‚¹ï¼š**
- å°batch sizeï¼Œå‡å°‘å†…å­˜å ç”¨
- ä¸ä½¿ç”¨DataLoader workersï¼ˆé¿å…å†…å­˜çˆ†ç‚¸ï¼‰
- é™åˆ¶500åªè‚¡ç¥¨ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
- ä½¿ç”¨LSTMï¼ˆæ¯”Transformerçœå†…å­˜ï¼‰

### é…ç½®2ï¼šå¹³è¡¡é…ç½®

```bash
python scripts/train_universal_model.py \
    --model-type lstm \
    --epochs 50 \
    --batch-size 256 \
    --hidden-size 128 \
    --device cuda \
    --amp \
    --num-workers 0 \
    --limit 1000
```

**ç‰¹ç‚¹ï¼š**
- ä¸­ç­‰batch size
- å¯ç”¨æ··åˆç²¾åº¦ï¼ˆå¦‚æœROCmæ”¯æŒï¼‰
- ä¸ä½¿ç”¨workers
- 1000åªè‚¡ç¥¨

### é…ç½®3ï¼šå…¨é‡è®­ç»ƒ

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 256 \
    --hidden-size 256 \
    --device cuda \
    --amp \
    --num-workers 0
```

**ç‰¹ç‚¹ï¼š**
- å…¨é‡è‚¡ç¥¨
- Transformeræ¨¡å‹
- æ··åˆç²¾åº¦
- ä¸ä½¿ç”¨workersï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰

---

## ğŸ’¡ å†…å­˜ä¼˜åŒ–æŠ€å·§

### é—®é¢˜ï¼šæ•°æ®åŠ è½½å ç”¨50%å†…å­˜

**åŸå› ï¼š** ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®åˆ°å†…å­˜

**è§£å†³æ–¹æ¡ˆï¼š**

#### æ–¹æ¡ˆ1ï¼šé™åˆ¶è‚¡ç¥¨æ•°é‡

```bash
# å…ˆç”¨500åªè‚¡ç¥¨æµ‹è¯•
python scripts/train_universal_model.py \
    --device cuda \
    --limit 500
```

#### æ–¹æ¡ˆ2ï¼šåˆ†æ‰¹è®­ç»ƒ

```bash
# è®­ç»ƒå‰1000åª
python scripts/train_universal_model.py \
    --device cuda \
    --limit 1000 \
    --output-dir out/universal_model_batch1

# è®­ç»ƒç¬¬1000-2000åª
python scripts/train_universal_model.py \
    --device cuda \
    --limit 2000 \
    --output-dir out/universal_model_batch2
```

#### æ–¹æ¡ˆ3ï¼šç¦ç”¨DataLoader workers

```bash
# workers=0 é¿å…å¤šè¿›ç¨‹å†…å­˜å¤åˆ¶
python scripts/train_universal_model.py \
    --device cuda \
    --num-workers 0
```

#### æ–¹æ¡ˆ4ï¼šå‡å°batch size

```bash
# ä½¿ç”¨æ›´å°çš„batch size
python scripts/train_universal_model.py \
    --device cuda \
    --batch-size 64
```

---

## ğŸ” ç›‘æ§å’Œè¯Šæ–­

### ç›‘æ§AMD GPUä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§GPU
watch -n 1 rocm-smi

# æˆ–è€…
watch -n 1 "rocm-smi | grep -A 10 'GPU'"
```

**å…³é”®æŒ‡æ ‡ï¼š**
- GPUä½¿ç”¨ç‡ï¼šåº”è¯¥æ¥è¿‘100%
- æ˜¾å­˜ä½¿ç”¨ï¼šåº”è¯¥å ç”¨70-90%
- æ¸©åº¦ï¼šåº”è¯¥åœ¨70-85Â°C

### ç›‘æ§å†…å­˜ä½¿ç”¨

```bash
# ç›‘æ§ç³»ç»Ÿå†…å­˜
watch -n 1 free -h

# ç›‘æ§è¿›ç¨‹å†…å­˜
watch -n 1 "ps aux | grep train_universal_model"
```

### è¯Šæ–­è„šæœ¬

åˆ›å»º `diagnose_gpu.py`ï¼š

```python
"""è¯Šæ–­GPUé…ç½®"""
import torch
import sys

print("="*70)
print("  PyTorch GPU è¯Šæ–­")
print("="*70)

# PyTorchç‰ˆæœ¬
print(f"\nPyTorchç‰ˆæœ¬: {torch.__version__}")

# CUDA/ROCmå¯ç”¨æ€§
cuda_available = torch.cuda.is_available()
print(f"CUDA/ROCmå¯ç”¨: {cuda_available}")

if cuda_available:
    # è®¾å¤‡ä¿¡æ¯
    device_count = torch.cuda.device_count()
    print(f"GPUæ•°é‡: {device_count}")
    
    for i in range(device_count):
        print(f"\nGPU {i}:")
        print(f"  åç§°: {torch.cuda.get_device_name(i)}")
        print(f"  æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
        
    # æµ‹è¯•GPUè®¡ç®—
    print("\næµ‹è¯•GPUè®¡ç®—...")
    try:
        x = torch.randn(1000, 1000).cuda()
        y = torch.randn(1000, 1000).cuda()
        z = torch.matmul(x, y)
        print("âœ… GPUè®¡ç®—æµ‹è¯•æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
else:
    print("\nâŒ CUDA/ROCmä¸å¯ç”¨ï¼")
    print("\nå¯èƒ½çš„åŸå› ï¼š")
    print("1. PyTorchæ²¡æœ‰å®‰è£…ROCmç‰ˆæœ¬")
    print("2. ROCmé©±åŠ¨æœªæ­£ç¡®å®‰è£…")
    print("3. ç¯å¢ƒå˜é‡æœªæ­£ç¡®è®¾ç½®")
    print("\nè§£å†³æ–¹æ¡ˆï¼š")
    print("pip uninstall torch -y")
    print("pip install torch --index-url https://download.pytorch.org/whl/rocm5.6")
    sys.exit(1)

print("\n" + "="*70)
```

è¿è¡Œè¯Šæ–­ï¼š

```bash
python diagnose_gpu.py
```

---

## ğŸ“Š AMD GPU vs NVIDIA GPU

### APIå…¼å®¹æ€§

| åŠŸèƒ½ | NVIDIA (CUDA) | AMD (ROCm) | è¯´æ˜ |
|------|--------------|-----------|------|
| è®¾å¤‡å‚æ•° | `--device cuda` | `--device cuda` | âœ… ç›¸åŒ |
| æ··åˆç²¾åº¦ | `--amp` | `--amp` | âš ï¸ éƒ¨åˆ†æ”¯æŒ |
| æ˜¾å­˜ç®¡ç† | è‡ªåŠ¨ | è‡ªåŠ¨ | âœ… ç›¸åŒ |
| ç›‘æ§å·¥å…· | `nvidia-smi` | `rocm-smi` | âŒ ä¸åŒ |

### æ€§èƒ½å·®å¼‚

| æŒ‡æ ‡ | NVIDIA RTX 4090 | AMD RX 7900 XTX | è¯´æ˜ |
|------|----------------|-----------------|------|
| FP32æ€§èƒ½ | 82.6 TFLOPS | 61 TFLOPS | NVIDIAæ›´å¿« |
| FP16æ€§èƒ½ | 165 TFLOPS | 122 TFLOPS | NVIDIAæ›´å¿« |
| æ˜¾å­˜ | 24GB | 24GB | ç›¸åŒ |
| è½¯ä»¶æ”¯æŒ | ä¼˜ç§€ | è‰¯å¥½ | NVIDIAæ›´æˆç†Ÿ |

---

## ğŸ¯ å®Œæ•´è®­ç»ƒæµç¨‹

### ç¬¬1æ­¥ï¼šç¯å¢ƒæ£€æŸ¥

```bash
# 1. æ£€æŸ¥ROCm
rocm-smi

# 2. æ£€æŸ¥PyTorch
python diagnose_gpu.py

# 3. æ£€æŸ¥ç¼“å­˜æ•°æ®
ls data/parquet/*.parquet | wc -l
ls data/features/*_features.parquet | wc -l
```

### ç¬¬2æ­¥ï¼šå°è§„æ¨¡æµ‹è¯•

```bash
# ä½¿ç”¨100åªè‚¡ç¥¨å¿«é€Ÿæµ‹è¯•
python scripts/train_universal_model.py \
    --device cuda \
    --limit 100 \
    --epochs 5 \
    --batch-size 128 \
    --num-workers 0
```

**é¢„æœŸè¾“å‡ºï¼š**
```
é…ç½®:
  ...
  è®¾å¤‡: cuda
  æ··åˆç²¾åº¦: ç¦ç”¨

åˆ›å»ºæ¨¡å‹...
âœ“ æ¨¡å‹å‚æ•°é‡: 2,345,678

å¼€å§‹è®­ç»ƒ...
======================================================================

Epoch [1/5] - Train Loss: 0.234567, Val Loss: 0.345678, Time: 12.34s
```

**å…³é”®æ£€æŸ¥ï¼š**
- âœ… è®¾å¤‡æ˜¾ç¤ºä¸º `cuda`
- âœ… æ¯ä¸ªepochæ—¶é—´åˆç†ï¼ˆ10-30ç§’ï¼‰
- âœ… æ²¡æœ‰å†…å­˜é”™è¯¯

### ç¬¬3æ­¥ï¼šä¸­ç­‰è§„æ¨¡æµ‹è¯•

```bash
# ä½¿ç”¨500åªè‚¡ç¥¨æµ‹è¯•
python scripts/train_universal_model.py \
    --device cuda \
    --limit 500 \
    --epochs 20 \
    --batch-size 256 \
    --num-workers 0
```

### ç¬¬4æ­¥ï¼šå…¨é‡è®­ç»ƒ

```bash
# å…¨é‡è®­ç»ƒ
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 256 \
    --hidden-size 256 \
    --device cuda \
    --num-workers 0
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: CUDAä¸å¯ç”¨æ€ä¹ˆåŠï¼Ÿ

**æ£€æŸ¥ï¼š**
```bash
python -c "import torch; print(torch.cuda.is_available())"
```

**å¦‚æœè¾“å‡º `False`ï¼š**
```bash
# é‡æ–°å®‰è£…PyTorch ROCmç‰ˆæœ¬
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6
```

### Q2: å†…å­˜å ç”¨å¤ªé«˜æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. é™åˆ¶è‚¡ç¥¨æ•°é‡
--limit 500

# 2. å‡å°batch size
--batch-size 64

# 3. ç¦ç”¨workers
--num-workers 0

# 4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
--hidden-size 64
```

### Q3: GPUåˆ©ç”¨ç‡ä½æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
1. æ•°æ®åŠ è½½å¤ªæ…¢ï¼ˆä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰
2. Batch sizeå¤ªå°
3. æ¨¡å‹å¤ªå°

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. ç¡®ä¿ä½¿ç”¨ç¼“å­˜
python scripts/prepare_training_data.py --symbols all --workers 8 --resume

# 2. å¢å¤§batch size
--batch-size 512

# 3. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
--model-type transformer --hidden-size 256
```

### Q4: æ··åˆç²¾åº¦ä¸æ”¯æŒæ€ä¹ˆåŠï¼Ÿ

**ç—‡çŠ¶ï¼š**
```
RuntimeError: "LayerNormKernelImpl" not implemented for 'Half'
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# ä¸ä½¿ç”¨æ··åˆç²¾åº¦
python scripts/train_universal_model.py \
    --device cuda \
    --batch-size 256
    # ä¸è¦åŠ  --amp
```

---

## ğŸ“ æ¨èé…ç½®æ€»ç»“

### å¿«é€Ÿæµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰

```bash
python scripts/train_universal_model.py \
    --device cuda \
    --limit 100 \
    --epochs 5 \
    --batch-size 128 \
    --num-workers 0
```

### ä¸­ç­‰è§„æ¨¡ï¼ˆ30åˆ†é’Ÿï¼‰

```bash
python scripts/train_universal_model.py \
    --device cuda \
    --limit 500 \
    --epochs 20 \
    --batch-size 256 \
    --num-workers 0
```

### å…¨é‡è®­ç»ƒï¼ˆ2-3å°æ—¶ï¼‰

```bash
python scripts/train_universal_model.py \
    --model-type transformer \
    --epochs 100 \
    --batch-size 256 \
    --hidden-size 256 \
    --device cuda \
    --num-workers 0
```

### å†…å­˜å—é™ï¼ˆæ¨èï¼‰

```bash
python scripts/train_universal_model.py \
    --model-type lstm \
    --epochs 50 \
    --batch-size 128 \
    --hidden-size 128 \
    --device cuda \
    --num-workers 0 \
    --limit 1000
```

---

## âœ¨ æ€»ç»“

### å…³é”®é…ç½®

1. âœ… **ä½¿ç”¨ `--device cuda`** - ROCmå…¼å®¹CUDA API
2. âœ… **ä½¿ç”¨ `--num-workers 0`** - é¿å…å†…å­˜çˆ†ç‚¸
3. âœ… **é™åˆ¶è‚¡ç¥¨æ•°é‡** - ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•
4. âœ… **ç›‘æ§GPUä½¿ç”¨** - ä½¿ç”¨ `rocm-smi`

### è¯Šæ–­æ¸…å•

- [ ] PyTorchæ”¯æŒROCmï¼ˆ`torch.cuda.is_available() == True`ï¼‰
- [ ] ç¼“å­˜æ•°æ®å·²å‡†å¤‡
- [ ] ä½¿ç”¨ `--device cuda` å‚æ•°
- [ ] ä½¿ç”¨ `--num-workers 0` é¿å…å†…å­˜é—®é¢˜
- [ ] ä»å°è§„æ¨¡æµ‹è¯•å¼€å§‹ï¼ˆ`--limit 100`ï¼‰

### ä¸‹ä¸€æ­¥

```bash
# 1. è¯Šæ–­GPU
python diagnose_gpu.py

# 2. å¿«é€Ÿæµ‹è¯•
python scripts/train_universal_model.py --device cuda --limit 100 --epochs 5 --num-workers 0

# 3. ç›‘æ§GPU
watch -n 1 rocm-smi
```

ç°åœ¨ä½ å¯ä»¥åœ¨AMD GPUä¸Šé«˜æ•ˆè®­ç»ƒäº†ï¼ğŸš€
