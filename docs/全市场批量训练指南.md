# å…¨å¸‚åœºæ‰¹é‡è®­ç»ƒæŒ‡å—

## ğŸ¯ ç›®æ ‡

æ‰¹é‡è®­ç»ƒå…¨å¸‚åœºæ‰€æœ‰è‚¡ç¥¨çš„AIé¢„æµ‹æ¨¡å‹ï¼Œå®ç°ï¼š
- âœ… è‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹
- âœ… å¤šè¿›ç¨‹å¹¶å‘è®­ç»ƒ
- âœ… æ–­ç‚¹ç»­ä¼ æ”¯æŒ
- âœ… è¿›åº¦è·Ÿè¸ªå’Œç»Ÿè®¡
- âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•

---

## ğŸ“‹ å®Œæ•´æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ•°æ®é¢„çƒ­ï¼ˆå¿…é¡»ï¼‰

æ‰¹é‡è®­ç»ƒå‰ï¼Œå¼ºçƒˆå»ºè®®å…ˆé¢„çƒ­å…¨å¸‚åœºæ•°æ®ï¼Œè¿™ä¼šå¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦ã€‚

```bash
# é¢„çƒ­æ‰€æœ‰è‚¡ç¥¨çš„Kçº¿å’Œç‰¹å¾æ•°æ®
python scripts/prepare_training_data.py \
    --symbols all \
    --workers 8 \
    --resume
```

**é¢„è®¡æ—¶é—´**ï¼š
- 5000åªè‚¡ç¥¨
- 8è¿›ç¨‹å¹¶å‘
- çº¦2-3å°æ—¶

**ä¸ºä»€ä¹ˆå¿…é¡»é¢„çƒ­ï¼Ÿ**
- ä¸é¢„çƒ­ï¼šæ¯åªè‚¡ç¥¨è®­ç»ƒéœ€è¦5-10åˆ†é’Ÿï¼ˆåŒ…å«æ•°æ®åŠ è½½å’Œç‰¹å¾è®¡ç®—ï¼‰
- é¢„çƒ­åï¼šæ¯åªè‚¡ç¥¨è®­ç»ƒåªéœ€30ç§’-2åˆ†é’Ÿï¼ˆåªéœ€è®­ç»ƒæ¨¡å‹ï¼‰
- é€Ÿåº¦æå‡ï¼š10-20å€

### ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è®­ç»ƒ

#### æ–¹æ¡ˆ1ï¼šå•è¿›ç¨‹è®­ç»ƒï¼ˆç¨³å®šï¼Œæ¨èæ–°æ‰‹ï¼‰

```bash
# è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨ï¼Œå•è¿›ç¨‹
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 1 \
    --resume
```

**ç‰¹ç‚¹**ï¼š
- âœ… ç¨³å®šå¯é 
- âœ… å†…å­˜å ç”¨ä½
- âœ… æ˜“äºè°ƒè¯•
- âŒ é€Ÿåº¦è¾ƒæ…¢

**é¢„è®¡æ—¶é—´**ï¼š
- 5000åªè‚¡ç¥¨
- æ¯åª2åˆ†é’Ÿ
- çº¦167å°æ—¶ï¼ˆ7å¤©ï¼‰

#### æ–¹æ¡ˆ2ï¼šå¤šè¿›ç¨‹å¹¶å‘è®­ç»ƒï¼ˆå¿«é€Ÿï¼Œæ¨èï¼‰

```bash
# ä½¿ç”¨4ä¸ªè¿›ç¨‹å¹¶å‘è®­ç»ƒ
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --resume
```

**ç‰¹ç‚¹**ï¼š
- âœ… é€Ÿåº¦å¿«ï¼ˆ4å€æå‡ï¼‰
- âœ… å……åˆ†åˆ©ç”¨CPU
- âœ… æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- âš ï¸  å†…å­˜å ç”¨è¾ƒé«˜

**é¢„è®¡æ—¶é—´**ï¼š
- 5000åªè‚¡ç¥¨
- 4è¿›ç¨‹å¹¶å‘
- çº¦42å°æ—¶ï¼ˆ1.75å¤©ï¼‰

#### æ–¹æ¡ˆ3ï¼šGPUåŠ é€Ÿè®­ç»ƒï¼ˆæœ€å¿«ï¼‰

```bash
# ç¡®ä¿å®‰è£…äº†CUDAç‰ˆæœ¬çš„PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118

# ä½¿ç”¨2ä¸ªè¿›ç¨‹ï¼ˆGPUè®­ç»ƒä¸éœ€è¦å¤ªå¤šè¿›ç¨‹ï¼‰
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 2 \
    --resume
```

**ç‰¹ç‚¹**ï¼š
- âœ… é€Ÿåº¦æœ€å¿«ï¼ˆ10-20å€æå‡ï¼‰
- âœ… é€‚åˆå¤§æ¨¡å‹
- âš ï¸  éœ€è¦GPUç¡¬ä»¶
- âš ï¸  GPUå†…å­˜é™åˆ¶

**é¢„è®¡æ—¶é—´**ï¼š
- 5000åªè‚¡ç¥¨
- GPUè®­ç»ƒ
- çº¦8-15å°æ—¶

### ç¬¬ä¸‰æ­¥ï¼šç›‘æ§è¿›åº¦

#### å®æ—¶æŸ¥çœ‹

è®­ç»ƒè¿‡ç¨‹ä¼šå®æ—¶æ˜¾ç¤ºè¿›åº¦ï¼š

```
======================================================================
  æ‰¹é‡è®­ç»ƒå…¨å¸‚åœºè‚¡ç¥¨æ¨¡å‹
======================================================================

è®­ç»ƒé…ç½®:
  æ¨¡å‹ç±»å‹: lstm
  åºåˆ—é•¿åº¦: 60
  è®­ç»ƒè½®æ•°: 50
  æ‰¹æ¬¡å¤§å°: 32
  å¹¶å‘è¿›ç¨‹: 4
  æ–­ç‚¹ç»­ä¼ : æ˜¯

è·å–æ‰€æœ‰æ´»è·ƒè‚¡ç¥¨...
æ‰¾åˆ° 5000 åªæ´»è·ƒè‚¡ç¥¨

å¾…è®­ç»ƒè‚¡ç¥¨: 5000 åª

======================================================================
  å¼€å§‹æ‰¹é‡è®­ç»ƒ
======================================================================

[1/5000] å¼€å§‹è®­ç»ƒ: 000001
âœ“ åŠ è½½æ•°æ®: 726 æ¡è®°å½•
âœ“ è®­ç»ƒé›†: 393 æ ·æœ¬
âœ“ éªŒè¯é›†: 84 æ ·æœ¬
âœ“ æµ‹è¯•é›†: 84 æ ·æœ¬
âœ“ è®­ç»ƒå®Œæˆ
âœ“ æœ€ä½³éªŒè¯æŸå¤±: 104.861324
âœ“ è¯„ä¼°æŒ‡æ ‡:
  MAE: 0.123456
  RMSE: 0.234567
  RÂ²: 0.95

[1/5000] âœ“ 000001 è®­ç»ƒæˆåŠŸ
  MAE: 0.123456, RMSE: 0.234567, RÂ²: 0.950000

[2/5000] å¼€å§‹è®­ç»ƒ: 000002
...
```

#### æŸ¥çœ‹è¿›åº¦æ–‡ä»¶

```bash
# æŸ¥çœ‹è¿›åº¦
cat out/.batch_training_progress.json

# è¾“å‡ºç¤ºä¾‹
{
  "completed": ["000001", "000002", "600519"],
  "failed": ["000003"],
  "timestamp": "2024-11-19T10:30:00"
}
```

#### ç»Ÿè®¡å·²å®Œæˆæ•°é‡

```bash
# ç»Ÿè®¡å·²å®Œæˆçš„æ¨¡å‹æ•°é‡
ls out/*/checkpoints/best_model.pth | wc -l

# æŸ¥çœ‹æœ€è¿‘è®­ç»ƒçš„æ¨¡å‹
ls -lt out/ | head -20
```

### ç¬¬å››æ­¥ï¼šå¤„ç†å¤±è´¥çš„è‚¡ç¥¨

è®­ç»ƒå®Œæˆåï¼Œå¯èƒ½æœ‰éƒ¨åˆ†è‚¡ç¥¨è®­ç»ƒå¤±è´¥ï¼ˆæ•°æ®ä¸è¶³ã€è´¨é‡é—®é¢˜ç­‰ï¼‰ã€‚

#### æŸ¥çœ‹å¤±è´¥åˆ—è¡¨

```bash
# ä»è¿›åº¦æ–‡ä»¶ä¸­æå–å¤±è´¥çš„è‚¡ç¥¨
cat out/.batch_training_progress.json | grep -A 100 '"failed"'
```

#### é‡æ–°è®­ç»ƒå¤±è´¥çš„è‚¡ç¥¨

```bash
# æ–¹æ³•1ï¼šæ‰‹åŠ¨æŒ‡å®šå¤±è´¥çš„è‚¡ç¥¨
python scripts/batch_train_all_stocks.py \
    --symbols 000003,000005,000010 \
    --workers 2

# æ–¹æ³•2ï¼šä½¿ç”¨è„šæœ¬æå–å¤±è´¥åˆ—è¡¨
python -c "
import json
with open('out/.batch_training_progress.json', 'r') as f:
    data = json.load(f)
    failed = ','.join(data.get('failed', []))
    print(failed)
" | xargs -I {} python scripts/batch_train_all_stocks.py --symbols {}
```

---

## ğŸš€ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šé¦–æ¬¡å…¨å¸‚åœºè®­ç»ƒ

```bash
# ç¬¬1æ­¥ï¼šæ•°æ®é¢„çƒ­ï¼ˆå¿…é¡»ï¼‰
python scripts/prepare_training_data.py \
    --symbols all \
    --workers 8 \
    --resume

# ç¬¬2æ­¥ï¼šæ‰¹é‡è®­ç»ƒ
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --resume
```

**é¢„è®¡æ€»æ—¶é—´**ï¼š
- æ•°æ®é¢„çƒ­ï¼š2-3å°æ—¶
- æ‰¹é‡è®­ç»ƒï¼š40-50å°æ—¶
- æ€»è®¡ï¼šçº¦2å¤©

### åœºæ™¯2ï¼šå¢é‡è®­ç»ƒæ–°è‚¡ç¥¨

```bash
# åªè®­ç»ƒæ–°å¢çš„è‚¡ç¥¨
python scripts/batch_train_all_stocks.py \
    --symbols 688001,688002,688003 \
    --workers 2
```

### åœºæ™¯3ï¼šé‡æ–°è®­ç»ƒç‰¹å®šæ¿å—

```bash
# è®­ç»ƒç§‘åˆ›æ¿è‚¡ç¥¨
python scripts/batch_train_all_stocks.py \
    --symbols 688001,688002,688003,688005,688008 \
    --workers 4
```

### åœºæ™¯4ï¼šæµ‹è¯•è®­ç»ƒæµç¨‹

```bash
# å…ˆç”¨å°‘é‡è‚¡ç¥¨æµ‹è¯•
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --limit 10 \
    --workers 2
```

### åœºæ™¯5ï¼šåå°è¿è¡Œï¼ˆLinux/Macï¼‰

```bash
# ä½¿ç”¨nohupåå°è¿è¡Œ
nohup python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --resume \
    > logs/batch_training_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# æŸ¥çœ‹è¿›åº¦
tail -f logs/batch_training_*.log

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep batch_train_all_stocks
```

### åœºæ™¯6ï¼šå®šæ—¶é‡è®­ç»ƒ

åˆ›å»ºå®šæ—¶ä»»åŠ¡ï¼Œæ¯å‘¨é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼š

```bash
# åˆ›å»ºè®­ç»ƒè„šæœ¬ weekly_retrain.sh
#!/bin/bash
echo "å¼€å§‹æ¯å‘¨é‡è®­ç»ƒ: $(date)"

# æ¸…é™¤æ—§è¿›åº¦
rm out/.batch_training_progress.json

# æ•°æ®é¢„çƒ­
python scripts/prepare_training_data.py \
    --symbols all \
    --workers 8 \
    --resume

# æ‰¹é‡è®­ç»ƒ
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --resume

echo "æ¯å‘¨é‡è®­ç»ƒå®Œæˆ: $(date)"
```

```bash
# æ·»åŠ åˆ°crontabï¼ˆæ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œï¼‰
0 2 * * 0 /path/to/weekly_retrain.sh >> /logs/weekly_retrain.log 2>&1
```

---

## âš™ï¸ å‚æ•°é…ç½®

### å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | ç¤ºä¾‹ |
|------|------|--------|------|
| `--symbols` | è‚¡ç¥¨ä»£ç åˆ—è¡¨ | `all` | `--symbols 000001,600519` |
| `--model-type` | æ¨¡å‹ç±»å‹ | `lstm` | `--model-type gru` |
| `--workers` | å¹¶å‘è¿›ç¨‹æ•° | `1` | `--workers 4` |
| `--resume` | æ–­ç‚¹ç»­ä¼  | `False` | `--resume` |
| `--output-dir` | è¾“å‡ºç›®å½• | `out` | `--output-dir models` |
| `--epochs` | è®­ç»ƒè½®æ•° | `50` | `--epochs 100` |
| `--batch-size` | æ‰¹æ¬¡å¤§å° | `32` | `--batch-size 64` |
| `--limit` | é™åˆ¶æ•°é‡ | `None` | `--limit 100` |

### è®­ç»ƒé…ç½®

åœ¨è„šæœ¬ä¸­ä¿®æ”¹`config`å­—å…¸ï¼š

```python
config = {
    "model_type": "lstm",           # æ¨¡å‹ç±»å‹: lstm/gru/transformer
    "seq_length": 60,               # åºåˆ—é•¿åº¦
    "hidden_size": 128,             # éšè—å±‚å¤§å°
    "num_layers": 2,                # å±‚æ•°
    "dropout": 0.2,                 # Dropout
    "epochs": 50,                   # è®­ç»ƒè½®æ•°
    "batch_size": 32,               # æ‰¹æ¬¡å¤§å°
    "learning_rate": 0.001,         # å­¦ä¹ ç‡
    "patience": 10,                 # æ—©åœè€å¿ƒå€¼
    "train_start_date": "2021-01-01",  # å¼€å§‹æ—¥æœŸ
    "train_end_date": "2024-12-31"     # ç»“æŸæ—¥æœŸ
}
```

### æ¨èé…ç½®

#### é…ç½®1ï¼šå¿«é€Ÿè®­ç»ƒï¼ˆCPUï¼‰

```bash
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --epochs 30 \
    --batch-size 16 \
    --resume
```

**ç‰¹ç‚¹**ï¼š
- è®­ç»ƒè½®æ•°å°‘ï¼ˆ30è½®ï¼‰
- æ‰¹æ¬¡å°ï¼ˆ16ï¼‰
- é€‚åˆCPUè®­ç»ƒ
- é€Ÿåº¦å¿«ä½†ç²¾åº¦ç•¥ä½

#### é…ç½®2ï¼šæ ‡å‡†è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --epochs 50 \
    --batch-size 32 \
    --resume
```

**ç‰¹ç‚¹**ï¼š
- å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
- é€‚åˆå¤§å¤šæ•°åœºæ™¯
- CPUæˆ–GPUå‡å¯

#### é…ç½®3ï¼šé«˜ç²¾åº¦è®­ç»ƒï¼ˆGPUï¼‰

```bash
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 2 \
    --epochs 100 \
    --batch-size 64 \
    --model-type transformer \
    --resume
```

**ç‰¹ç‚¹**ï¼š
- è®­ç»ƒè½®æ•°å¤šï¼ˆ100è½®ï¼‰
- æ‰¹æ¬¡å¤§ï¼ˆ64ï¼‰
- ä½¿ç”¨Transformeræ¨¡å‹
- éœ€è¦GPUæ”¯æŒ

---

## ğŸ“Š æ€§èƒ½ä¼°ç®—

### æ—¶é—´ä¼°ç®—

| é…ç½® | å•åªè‚¡ç¥¨ | 100åª | 1000åª | 5000åª |
|------|---------|-------|--------|--------|
| CPUå•è¿›ç¨‹ï¼ˆæ— é¢„çƒ­ï¼‰ | 8åˆ†é’Ÿ | 13å°æ—¶ | 133å°æ—¶ | 667å°æ—¶ |
| CPUå•è¿›ç¨‹ï¼ˆé¢„çƒ­åï¼‰ | 2åˆ†é’Ÿ | 3.3å°æ—¶ | 33å°æ—¶ | 167å°æ—¶ |
| CPU 4è¿›ç¨‹ï¼ˆé¢„çƒ­åï¼‰ | 2åˆ†é’Ÿ | 50åˆ†é’Ÿ | 8.3å°æ—¶ | 42å°æ—¶ |
| CPU 8è¿›ç¨‹ï¼ˆé¢„çƒ­åï¼‰ | 2åˆ†é’Ÿ | 25åˆ†é’Ÿ | 4.2å°æ—¶ | 21å°æ—¶ |
| GPUï¼ˆé¢„çƒ­åï¼‰ | 30ç§’ | 50åˆ†é’Ÿ | 8.3å°æ—¶ | 42å°æ—¶ |

### èµ„æºå ç”¨

| é…ç½® | CPUä½¿ç”¨ç‡ | å†…å­˜ä½¿ç”¨ | ç£ç›˜ç©ºé—´ |
|------|----------|---------|----------|
| å•è¿›ç¨‹ | 25% | 2GB | æ¯åªçº¦10MB |
| 4è¿›ç¨‹ | 80% | 8GB | æ¯åªçº¦10MB |
| 8è¿›ç¨‹ | 95% | 16GB | æ¯åªçº¦10MB |
| GPUè®­ç»ƒ | 30% | 4GB + 4GBæ˜¾å­˜ | æ¯åªçº¦10MB |

### ç£ç›˜ç©ºé—´éœ€æ±‚

```
5000åªè‚¡ç¥¨ Ã— 10MB/åª = 50GB

å»ºè®®é¢„ç•™ï¼š100GB
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šå†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼š
```
MemoryError: Unable to allocate array
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# æ–¹æ³•1ï¼šå‡å°‘å¹¶å‘æ•°
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 2 \  # ä»4å‡å°‘åˆ°2
    --resume

# æ–¹æ³•2ï¼šå‡å°æ‰¹æ¬¡å¤§å°
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --batch-size 16 \  # ä»32å‡å°‘åˆ°16
    --resume
```

### é—®é¢˜2ï¼šè®­ç»ƒä¸­æ–­

**ç—‡çŠ¶**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ–­ï¼ˆæ–­ç”µã€ç½‘ç»œæ–­å¼€ç­‰ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# ä½¿ç”¨æ–­ç‚¹ç»­ä¼ é‡æ–°å¼€å§‹
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --resume  # ä¼šè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„è‚¡ç¥¨
```

### é—®é¢˜3ï¼šéƒ¨åˆ†è‚¡ç¥¨è®­ç»ƒå¤±è´¥

**ç—‡çŠ¶**ï¼šæŸäº›è‚¡ç¥¨æ•°æ®ä¸è¶³æˆ–è´¨é‡é—®é¢˜å¯¼è‡´è®­ç»ƒå¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# æŸ¥çœ‹å¤±è´¥åˆ—è¡¨
cat out/.batch_training_progress.json

# å¤±è´¥çš„è‚¡ç¥¨ä¼šè¢«è®°å½•ï¼Œå¯ä»¥å•ç‹¬å¤„ç†æˆ–å¿½ç•¥
# é€šå¸¸æ˜¯å› ä¸ºï¼š
# 1. æ•°æ®é‡ä¸è¶³ï¼ˆ<200æ¡ï¼‰
# 2. æ•°æ®è´¨é‡é—®é¢˜ï¼ˆå…¨æ˜¯NaNï¼‰
# 3. æ–°è‚¡ä¸Šå¸‚æ—¶é—´çŸ­
```

### é—®é¢˜4ï¼šç£ç›˜ç©ºé—´ä¸è¶³

**ç—‡çŠ¶**ï¼š
```
OSError: [Errno 28] No space left on device
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# æ–¹æ³•1ï¼šæ¸…ç†æ—§æ¨¡å‹
rm -rf out/*/checkpoints/checkpoint_epoch_*.pth

# æ–¹æ³•2ï¼šåªä¿ç•™æœ€ä½³æ¨¡å‹
find out -name "checkpoint_epoch_*.pth" -delete

# æ–¹æ³•3ï¼šä½¿ç”¨å…¶ä»–ç£ç›˜
python scripts/batch_train_all_stocks.py \
    --symbols all \
    --output-dir /other/disk/models \
    --resume
```

### é—®é¢˜5ï¼šè®­ç»ƒé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# 1. ç¡®ä¿å·²ç»é¢„çƒ­æ•°æ®
python scripts/prepare_training_data.py --symbols all --workers 8 --resume

# 2. å¢åŠ å¹¶å‘æ•°
python scripts/batch_train_all_stocks.py --symbols all --workers 8 --resume

# 3. ä½¿ç”¨GPU
pip install torch --index-url https://download.pytorch.org/whl/cu118
python scripts/batch_train_all_stocks.py --symbols all --workers 2 --resume

# 4. å‡å°‘è®­ç»ƒè½®æ•°
python scripts/batch_train_all_stocks.py --symbols all --epochs 30 --resume
```

---

## ğŸ“ˆ ç›‘æ§å’Œç»Ÿè®¡

### å®æ—¶ç›‘æ§è„šæœ¬

åˆ›å»º `monitor_training.sh`ï¼š

```bash
#!/bin/bash
# ç›‘æ§æ‰¹é‡è®­ç»ƒè¿›åº¦

echo "=== æ‰¹é‡è®­ç»ƒè¿›åº¦ç›‘æ§ ==="
echo ""

# ç»Ÿè®¡å·²å®Œæˆæ•°é‡
completed=$(cat out/.batch_training_progress.json 2>/dev/null | grep -o '"completed"' | wc -l)
failed=$(cat out/.batch_training_progress.json 2>/dev/null | grep -o '"failed"' | wc -l)

echo "å·²å®Œæˆ: $completed åª"
echo "å·²å¤±è´¥: $failed åª"

# ç»Ÿè®¡æ¨¡å‹æ–‡ä»¶
models=$(find out -name "best_model.pth" | wc -l)
echo "æ¨¡å‹æ–‡ä»¶: $models ä¸ª"

# ç£ç›˜ä½¿ç”¨
echo ""
echo "ç£ç›˜ä½¿ç”¨:"
du -sh out/

# è¿›ç¨‹çŠ¶æ€
echo ""
echo "è¿è¡Œä¸­çš„è¿›ç¨‹:"
ps aux | grep batch_train_all_stocks | grep -v grep || echo "  æ— è¿è¡Œä¸­çš„è¿›ç¨‹"

# æœ€è¿‘è®­ç»ƒçš„è‚¡ç¥¨
echo ""
echo "æœ€è¿‘è®­ç»ƒçš„5åªè‚¡ç¥¨:"
ls -lt out/ | head -6 | tail -5 | awk '{print "  " $9}'
```

```bash
# ä½¿ç”¨
chmod +x monitor_training.sh
./monitor_training.sh

# æˆ–å®šæ—¶ç›‘æ§
watch -n 60 ./monitor_training.sh
```

### ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š

åˆ›å»º `generate_report.py`ï¼š

```python
"""ç”Ÿæˆæ‰¹é‡è®­ç»ƒæŠ¥å‘Š"""
import json
from pathlib import Path
from datetime import datetime

def generate_report():
    # è¯»å–è¿›åº¦æ–‡ä»¶
    progress_file = "out/.batch_training_progress.json"
    if not Path(progress_file).exists():
        print("è¿›åº¦æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    with open(progress_file, 'r') as f:
        progress = json.load(f)
    
    completed = progress.get('completed', [])
    failed = progress.get('failed', [])
    
    # ç»Ÿè®¡
    total = len(completed) + len(failed)
    success_rate = len(completed) / total * 100 if total > 0 else 0
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""
# æ‰¹é‡è®­ç»ƒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ€»ä½“ç»Ÿè®¡

- æ€»è‚¡ç¥¨æ•°: {total}
- æˆåŠŸ: {len(completed)} ({success_rate:.1f}%)
- å¤±è´¥: {len(failed)} ({100-success_rate:.1f}%)

## æˆåŠŸåˆ—è¡¨

{chr(10).join(f'- {s}' for s in completed[:20])}
{'...' if len(completed) > 20 else ''}

## å¤±è´¥åˆ—è¡¨

{chr(10).join(f'- {s}' for s in failed[:20])}
{'...' if len(failed) > 20 else ''}

## æ¨¡å‹æ–‡ä»¶

- è¾“å‡ºç›®å½•: out/
- æ¨¡å‹æ•°é‡: {len(list(Path('out').glob('*/checkpoints/best_model.pth')))}
- ç£ç›˜å ç”¨: {sum(f.stat().st_size for f in Path('out').rglob('*') if f.is_file()) / 1024 / 1024 / 1024:.2f} GB
"""
    
    # ä¿å­˜æŠ¥å‘Š
    with open('out/training_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(report)
    print("\næŠ¥å‘Šå·²ä¿å­˜åˆ°: out/training_report.md")

if __name__ == "__main__":
    generate_report()
```

```bash
# ç”ŸæˆæŠ¥å‘Š
python generate_report.py
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. åˆ†é˜¶æ®µæ‰§è¡Œ

```bash
# é˜¶æ®µ1ï¼šæ•°æ®é¢„çƒ­ï¼ˆå‘¨æœ«æ‰§è¡Œï¼‰
python scripts/prepare_training_data.py --symbols all --workers 8 --resume

# é˜¶æ®µ2ï¼šæ‰¹é‡è®­ç»ƒï¼ˆå·¥ä½œæ—¥åå°æ‰§è¡Œï¼‰
nohup python scripts/batch_train_all_stocks.py \
    --symbols all \
    --workers 4 \
    --resume \
    > logs/training.log 2>&1 &
```

### 2. å®šæœŸé‡è®­ç»ƒ

```bash
# æ¯æœˆé‡è®­ç»ƒä¸€æ¬¡
# 1. æ¸…é™¤æ—§è¿›åº¦
rm out/.batch_training_progress.json

# 2. é‡æ–°é¢„çƒ­æ•°æ®
python scripts/prepare_training_data.py --symbols all --workers 8 --resume

# 3. æ‰¹é‡è®­ç»ƒ
python scripts/batch_train_all_stocks.py --symbols all --workers 4 --resume
```

### 3. å¤‡ä»½é‡è¦æ¨¡å‹

```bash
# å¤‡ä»½æ‰€æœ‰æœ€ä½³æ¨¡å‹
mkdir -p backups/models_$(date +%Y%m%d)
find out -name "best_model.pth" -exec cp {} backups/models_$(date +%Y%m%d)/ \;
```

### 4. ç›‘æ§èµ„æºä½¿ç”¨

```bash
# ç›‘æ§CPUå’Œå†…å­˜
htop

# ç›‘æ§ç£ç›˜
df -h

# ç›‘æ§GPUï¼ˆå¦‚æœæœ‰ï¼‰
nvidia-smi -l 1
```

---

## ğŸ“ æ€»ç»“

### å®Œæ•´æµç¨‹å›é¡¾

```bash
# 1. æ•°æ®é¢„çƒ­ï¼ˆ2-3å°æ—¶ï¼‰
python scripts/prepare_training_data.py --symbols all --workers 8 --resume

# 2. æ‰¹é‡è®­ç»ƒï¼ˆ40-50å°æ—¶ï¼‰
python scripts/batch_train_all_stocks.py --symbols all --workers 4 --resume

# 3. ç”ŸæˆæŠ¥å‘Š
python generate_report.py

# 4. å¤‡ä»½æ¨¡å‹
mkdir -p backups/models_$(date +%Y%m%d)
find out -name "best_model.pth" -exec cp {} backups/models_$(date +%Y%m%d)/ \;
```

### å…³é”®è¦ç‚¹

1. âœ… **å¿…é¡»é¢„çƒ­æ•°æ®** - é€Ÿåº¦æå‡10-20å€
2. âœ… **ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ** - é¿å…é‡å¤è®­ç»ƒ
3. âœ… **åˆç†è®¾ç½®å¹¶å‘æ•°** - CPUæ ¸å¿ƒæ•°æˆ–ç•¥å°‘
4. âœ… **ç›‘æ§è¿›åº¦** - å®šæœŸæ£€æŸ¥è¿›åº¦æ–‡ä»¶
5. âœ… **å¤„ç†å¤±è´¥** - åˆ†æå¤±è´¥åŸå› ï¼Œå¿…è¦æ—¶é‡è¯•

### æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | é¢„çƒ­ | å¹¶å‘ | GPU | 5000åªè€—æ—¶ |
|------|------|------|-----|-----------|
| åŸºç¡€ | âŒ | 1 | âŒ | 667å°æ—¶ |
| ä¼˜åŒ–1 | âœ… | 1 | âŒ | 167å°æ—¶ |
| ä¼˜åŒ–2 | âœ… | 4 | âŒ | 42å°æ—¶ |
| ä¼˜åŒ–3 | âœ… | 8 | âŒ | 21å°æ—¶ |
| æœ€ä¼˜ | âœ… | 2 | âœ… | 8-15å°æ—¶ |

ç°åœ¨ä½ å¯ä»¥é«˜æ•ˆåœ°è®­ç»ƒå…¨å¸‚åœºæ‰€æœ‰è‚¡ç¥¨çš„AIæ¨¡å‹äº†ï¼ğŸš€
